{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDd3bc2LJZWcjqmxuqFcCU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raanggasa/Personal-Web/blob/main/Asosiasi_Sentimen_Youtube.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g29tWS64Hbdp",
        "outputId": "ce538b98-0724-4236-9c9b-12f18458b82b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.169.0)\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.11/dist-packages (0.23.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.24.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.5.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (11.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-api-python-client mlxtend plotly networkx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "\n",
        "# Ganti dengan API Key kamu sendiri, dan JANGAN disebarkan ke publik\n",
        "api_key = \"AIzaSyBM2an5JFKLx7jlXlvEPq5k8sMneW92Zho\" # Ganti dengan API Key kamu sendiri\n",
        "\n",
        "# ID video YouTube yang ingin diambil komentarnya\n",
        "# This video_id is for a public video on the Google Developers channel\n",
        "# Please ensure this video still exists and comments are enabled\n",
        "video_id = \"hHW1oY26kxQ\"  # Ganti dengan ID video lain jika perlu\n",
        "\n",
        "# Inisialisasi service YouTube API\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "# Variabel untuk menyimpan komentar\n",
        "comments = []\n",
        "\n",
        "# Ambil komentar pertama (max 5)\n",
        "request = youtube.commentThreads().list(\n",
        "    part='snippet',\n",
        "    videoId=video_id,\n",
        "    maxResults=5,\n",
        "    textFormat='plainText'\n",
        ")\n",
        "\n",
        "response = request.execute()\n",
        "\n",
        "# Ekstrak komentar dari response\n",
        "while response is not None:\n",
        "    for item in response['items']:\n",
        "        comment = item['snippet']['topLevelComment']['snippet'] # Accessing the comment data directly\n",
        "        comments.append({\n",
        "            'author' : comment['authorDisplayName'],\n",
        "            'text' : comment['textDisplay'],\n",
        "            'like' : comment['likeCount'],\n",
        "            'published' : comment['publishedAt']\n",
        "        })\n",
        "\n",
        "    # Corrected indentation for if statement to align with for loop\n",
        "    if 'nextPageToken' in response:\n",
        "        request = youtube.commentThreads().list (\n",
        "            part='snippet',\n",
        "            videoId=video_id,\n",
        "            pageToken=response['nextPageToken'],\n",
        "            maxResults=5,\n",
        "            textFormat='plainText'\n",
        "        )\n",
        "        response = request.execute()\n",
        "    else:\n",
        "        break\n",
        "\n",
        "df = pd.DataFrame(comments)\n",
        "print(df.head(5)) # Changed heada to head\n",
        "\n",
        "# Tampilkan hasil\n",
        "df.to_csv ('youtube_comments.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAcndJrbI-5I",
        "outputId": "397b8720-6745-4499-b573-4c3a9c9f42a0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 author                                               text  \\\n",
            "0             @LofiGirl  The stream is back ðŸ§¡\\nhttps://www.youtube.com/...   \n",
            "1   @LoveEverything-h4o                     Hey guys howâ€™s everyone doing?   \n",
            "2           @that1guy49                                        This is old   \n",
            "3  @russellbradwell8910  When a man is in the presence of a woman he be...   \n",
            "4       @thejohnsweeney                                        I miss her.   \n",
            "\n",
            "   like             published  \n",
            "0  4975  2020-02-22T20:28:18Z  \n",
            "1     1  2025-03-15T23:53:19Z  \n",
            "2     1  2025-03-01T17:10:47Z  \n",
            "3     0  2024-11-08T17:12:32Z  \n",
            "4     0  2024-09-06T00:29:47Z  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81soD19_WaQz",
        "outputId": "905644df-fae6-4d80-a5da-8b3272ed5688"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.11/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# 1. Load dataset\n",
        "# Ganti dengan path dataset kamu\n",
        "df = pd.read_csv(r'/content/youtube_comments.csv')\n",
        "\n",
        "# 2. Inisialisasi stemmer Sastrawi\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# 3. Stopwords tambahan\n",
        "custom_stopwords = set([\n",
        "    'dan', 'yang', 'di', 'ke', 'dari', 'untuk', 'dengan', 'nya', 'bang',\n",
        "    'g', 'yg', 'a', 'saya', 'kamu', 'kita', 'itu', 'ini', 'ada', 'lebih',\n",
        "    'jadi', 'beli', 'hp', 'video', 'aja', 'dong', 'udah', 'dah', 'lah',\n",
        "    'ga', 'enggak', 'buat', 'kok', 'loh', 'nih', 'si', 'kayak', 'sih',\n",
        "    'oh', 'gue', 'lu', 'klo', 'pdhl', 'pada', 'apa', 'nubia', 'david',\n",
        "    'gak', 'bisa'\n",
        "])\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Hapus karakter non-alfabet\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    # Tokenisasi (memisahkan kata)\n",
        "    words = text.split()\n",
        "\n",
        "    # Stopword removal\n",
        "    words = [word for word in words if word not in custom_stopwords]\n",
        "\n",
        "    # Stemming\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # Gabungkan kembali kata-kata menjadi kalimat\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Corrected indentation for these two lines to align with the script's main block\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "print(df[['author', 'processed_text', 'like', 'published']].head(5)) # Corrected column names and used double brackets for selection"
      ],
      "metadata": {
        "id": "fl0g8jNIQnll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kamus kata positif dan negatif\n",
        "positif_words = ['bagus', 'baik', 'mantap', 'rekomendasi', 'top', 'worth', 'suka']\n",
        "negatif_words = ['jelek', 'buruk', 'lemot', 'parah', 'sampah', 'menyesal']\n",
        "\n",
        "# Fungsi untuk mendapatkan sentimen\n",
        "def get_sentiment(text):\n",
        "    # Pastikan input adalah string dan bukan NaN\n",
        "    if not isinstance(text, str):\n",
        "        return 'netral' # Atau label default lainnya untuk nilai non-string\n",
        "\n",
        "    words = text.lower().split()\n",
        "\n",
        "    positif_score = sum(w in positif_words for w in words)\n",
        "    negatif_score = sum(w in negatif_words for w in words)\n",
        "\n",
        "    if positif_score > negatif_score:\n",
        "        return 'positif'\n",
        "    elif negatif_score > positif_score:\n",
        "        return 'negatif'\n",
        "    else:\n",
        "        return 'netral'\n",
        "\n",
        "# Terapkan fungsi ke kolom 'processed_text'\n",
        "# Gunakan .astype(str) untuk memastikan semua nilai dalam 'processed_text' adalah string\n",
        "df[\"label\"] = df['processed_text'].astype(str).apply(get_sentiment)\n",
        "\n",
        "\n",
        "# Menampilkan dataset dengan label sentimen\n",
        "print(df[['author', 'processed_text', 'label']].head())"
      ],
      "metadata": {
        "id": "xPsELqW-WybY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Membuat dictionary untuk menyimpan dataset per label\n",
        "sentimen_datasets = {}\n",
        "\n",
        "# Untuk setiap label sentimen\n",
        "for label in df[\"label\"].unique():\n",
        "    df_label = df[df[\"label\"] == label].copy() # Make a copy to avoid SettingWithCopyWarning\n",
        "\n",
        "    # Ensure 'processed_text' column is treated as string\n",
        "    df_label['processed_text'] = df_label['processed_text'].astype(str)\n",
        "\n",
        "    # Gunakan CountVectorizer untuk One-Hot Encoding\n",
        "    # Add min_df to ignore terms that appear in less than a minimum number of documents\n",
        "    vectorizer = CountVectorizer(binary=True, min_df=1)\n",
        "    X = vectorizer.fit_transform(df_label['processed_text'])\n",
        "\n",
        "    # Buat DataFrame dari hasil encoding\n",
        "    # Use df_label.index for index alignment\n",
        "    df_onehot = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out(), index=df_label.index)\n",
        "\n",
        "    # Simpan ke dictionary\n",
        "    sentimen_datasets[label] = df_onehot\n",
        "\n",
        "# Menampilkan hasil One-Hot Encoding untuk setiap sentimen\n",
        "for label, data in sentimen_datasets.items():\n",
        "    print(f\"\\nOne-Hot Encoding untuk label: {label}\")\n",
        "    print(data.head())"
      ],
      "metadata": {
        "id": "p7C9_pGmXOLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "\n",
        "# Pastikan df_onehot berisi nilai boolean (True/False)\n",
        "# Note: This assumes you are working with df_onehot from the *last* sentiment category processed.\n",
        "# If you intend to run association rule mining for each sentiment category,\n",
        "# you should iterate through the sentimen_datasets dictionary.\n",
        "# For this example, we will assume you want to run it on one of the df_onehot DataFrames.\n",
        "# Let's assume you want to run it on the 'positif' sentiment data for demonstration.\n",
        "# You would typically choose one sentiment category or combine them if needed.\n",
        "\n",
        "# Example using 'positif' sentiment data:\n",
        "# Check if 'positif' key exists in the dictionary\n",
        "if 'positif' in sentimen_datasets:\n",
        "    df_onehot_positif = sentimen_datasets['positif'].astype(bool)\n",
        "\n",
        "    # Menggunakan Apriori untuk menemukan frequent itemsets\n",
        "    frequent_itemsets_positif = apriori(df_onehot_positif, min_support=0.1, use_colnames=True)\n",
        "\n",
        "    # Menghasilkan aturan asosiasi berdasarkan frequent itemsets\n",
        "    rules_positif = association_rules(frequent_itemsets_positif, metric=\"lift\", min_threshold=1)\n",
        "\n",
        "    # Menampilkan aturan asosiasi untuk sentimen positif\n",
        "    print(\"\\nAssociation Rules for 'positif' sentiment:\")\n",
        "    print(rules_positif.head())\n",
        "else:\n",
        "    print(\"\\n'positif' sentiment data not found in sentimen_datasets.\")\n",
        "\n",
        "# If you want to run on a different sentiment category, change 'positif' accordingly.\n",
        "# If you want to run on the combined data from all sentiment categories,\n",
        "# you would first concatenate the DataFrames in sentimen_datasets."
      ],
      "metadata": {
        "id": "F0U5Z7bUZdVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ambil sebagian aturan untuk visualisasi (agar tidak terlalu ramai)\n",
        "# Pastikan variabel 'rules' yang digunakan di sini sesuai dengan association rules yang ingin Anda visualisasikan.\n",
        "# Jika Anda mengikuti saran di langkah sebelumnya, rules untuk sentimen positif tersimpan di 'rules_positif'.\n",
        "# Ganti 'rules' dengan nama variabel yang relevan jika Anda telah menganalisis association rules untuk kategori lain.\n",
        "# Contoh: rules_to_plot = rules_positif.iloc[0:10]\n",
        "try:\n",
        "    rules_to_plot = rules.iloc[0:10]\n",
        "except NameError:\n",
        "    print(\"Variabel 'rules' tidak ditemukan. Pastikan Anda telah menjalankan sel yang menghasilkan association rules (misalnya, 'rules_positif') dan gunakan nama variabel yang benar.\")\n",
        "    # Fallback or exit if 'rules' is not defined\n",
        "    # For demonstration, let's assume 'rules_positif' exists if 'rules' doesn't\n",
        "    if 'rules_positif' in locals():\n",
        "        print(\"Menggunakan 'rules_positif' sebagai gantinya.\")\n",
        "        rules_to_plot = rules_positif.iloc[0:10]\n",
        "    else:\n",
        "        print(\"Tidak ada variabel association rules yang ditemukan untuk divisualisasikan.\")\n",
        "        # Exit or handle the case where no rules are available\n",
        "        exit() # Or raise an error or return\n",
        "\n",
        "# Buat graph kosong\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Tambahkan edge dari antecedents ke consequents\n",
        "for _, row in rules_to_plot.iterrows():\n",
        "    for antecedent in row['antecedents']:\n",
        "        for consequent in row['consequents']:\n",
        "            G.add_edge(antecedent, consequent, weight=row['lift'])\n",
        "\n",
        "# Buat posisi node dengan spring layout\n",
        "pos = nx.spring_layout(G, k=1.5, seed=42)  # K=1.5 mengatur jarak antar node\n",
        "\n",
        "# Gambar graf\n",
        "plt.figure(figsize=(14, 10))  # Ukuran gambar diperbesar agar lebih jelas\n",
        "nx.draw(\n",
        "    G, pos, with_labels=True, node_size=3500,\n",
        "    node_color='lightblue', font_size=10, font_weight='bold',\n",
        "    edge_color='gray', arrowsize=20\n",
        ")\n",
        "plt.title('Visualisasi Association Rules', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JsyZR_gCahvT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}